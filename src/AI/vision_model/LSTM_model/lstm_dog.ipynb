{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.3 in c:\\users\\wjsgh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "   %pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 로딩 중...\n",
      "총 파일 수: 39537\n",
      "처리된 파일 수: 39537\n",
      "건너뛴 파일 수: 0\n",
      "클래스 목록: ['BODYLOWER', 'BODYSCRATCH', 'BODYSHAKE', 'FEETUP', 'FOOTUP', 'HEADING', 'LYING', 'MOUNTING', 'SIT', 'TAILING', 'TAILLOW', 'TURN', 'WALKRUN']\n",
      "\n",
      "검증 데이터 로딩 중...\n",
      "총 파일 수: 4949\n",
      "처리된 파일 수: 4949\n",
      "건너뛴 파일 수: 0\n",
      "클래스 목록: ['BODYLOWER', 'BODYSCRATCH', 'BODYSHAKE', 'FEETUP', 'FOOTUP', 'HEADING', 'LYING', 'MOUNTING', 'SIT', 'TAILING', 'TAILLOW', 'TURN', 'WALKRUN']\n",
      "\n",
      "로드된 학습 데이터 수: 39537\n",
      "로드된 검증 데이터 수: 4949\n",
      "총 클래스 수: 13\n",
      "클래스 목록: ['MOUNTING', 'FEETUP', 'BODYLOWER', 'TURN', 'SIT', 'BODYSHAKE', 'TAILING', 'HEADING', 'BODYSCRATCH', 'TAILLOW', 'LYING', 'WALKRUN', 'FOOTUP']\n",
      "최신 체크포인트 발견: improved_lstm_model_dog_epoch_90.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjsgh\\AppData\\Local\\Temp\\ipykernel_24176\\1659742754.py:273: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(latest_checkpoint, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트에서 학습 재개: 에포크 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 1/10 [03:06<27:54, 186.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 [91/100], 학습 손실: 1.6033, 학습 정확도: 49.13%, 검증 손실: 1.7102, 검증 정확도: 44.55%\n",
      "Validation loss decreased (inf --> 1.710159). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 2/10 [06:05<24:15, 181.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 [92/100], 학습 손실: 1.6014, 학습 정확도: 48.86%, 검증 손실: 1.7084, 검증 정확도: 44.37%\n",
      "Validation loss decreased (1.710159 --> 1.708443). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 3/10 [08:59<20:49, 178.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 [93/100], 학습 손실: 1.5951, 학습 정확도: 49.03%, 검증 손실: 1.7031, 검증 정확도: 44.66%\n",
      "Validation loss decreased (1.708443 --> 1.703091). Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 3/10 [09:59<23:17, 199.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 318\u001b[0m\n\u001b[0;32m    316\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_keypoints, batch_skeleton)\n\u001b[0;32m    317\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m--> 318\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    321\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\wjsgh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wjsgh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wjsgh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# device 정의\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DOG_SKELETON = [\n",
    "    [0, 1],  # 코에서 이마\n",
    "    [0, 2],  # 코에서 입꼬리\n",
    "    [2, 3],  # 입꼬리에서 아랫입술\n",
    "    [1, 4],  # 이마에서 목\n",
    "    [4, 5],  # 목에서 오른쪽 앞다리 시작점\n",
    "    [4, 6],  # 목에서 왼쪽 앞다리 시작점\n",
    "    [5, 7],  # 오른쪽 앞다리 시작점에서 오른쪽 앞발목\n",
    "    [6, 8],  # 왼쪽 앞다리 시작점에서 왼쪽 앞발목\n",
    "    [9, 11],  # 오른쪽 허벅지에서 오른쪽 뒷발목\n",
    "    [10, 12],  # 왼쪽 허벅지에서 왼쪽 뒷발목\n",
    "    [4, 13],  # 목에서 꼬리 시작점\n",
    "    [13, 14],  # 꼬리 시작점에서 꼬리 끝\n",
    "    [9, 13],  # 오른쪽 허벅지에서 꼬리 시작점 (오른쪽 하체)\n",
    "    [10, 13], # 왼쪽 허벅지에서 꼬리 시작점 (왼쪽 하체)\n",
    "    [5, 9],   # 오른쪽 앞다리 시작점에서 오른쪽 허벅지 (오른쪽 몸통)\n",
    "    [6, 10],  # 왼쪽 앞다리 시작점에서 왼쪽 허벅지 (왼쪽 몸통)\n",
    "    [5, 6],   # 오른쪽 앞다리 시작점에서 왼쪽 앞다리 시작점 (가슴 윤곽)\n",
    "    [9, 10]   # 오른쪽 허벅지에서 왼쪽 허벅지 (엉덩이 윤곽)\n",
    "]\n",
    "\n",
    "\n",
    "def extract_keypoints(keypoints):\n",
    "    extracted = []\n",
    "    for i in range(1, 16):  # 1부터 15까지의 키포인트\n",
    "        point = keypoints.get(str(i))\n",
    "        if point is not None:\n",
    "            extracted.extend([point['x'], point['y']])\n",
    "        else:\n",
    "            extracted.extend([0, 0])  # 없는 키포인트는 (0, 0)으로 처리\n",
    "    return extracted\n",
    "\n",
    "def extract_skeleton(keypoints):\n",
    "    skeleton = []\n",
    "    for start, end in DOG_SKELETON:\n",
    "        start_point = keypoints.get(str(start+1))\n",
    "        end_point = keypoints.get(str(end+1))\n",
    "        if start_point and end_point:\n",
    "            skeleton.extend([start_point['x'], start_point['y'], end_point['x'], end_point['y']])\n",
    "        else:\n",
    "            skeleton.extend([0, 0, 0, 0])\n",
    "    return skeleton\n",
    "\n",
    "def load_json_files(base_folder):\n",
    "    keypoints_data = []\n",
    "    skeleton_data = []\n",
    "    labels = []\n",
    "    metadata = []\n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    skipped_files = 0\n",
    "    class_names = []\n",
    "\n",
    "    for action_folder in os.listdir(base_folder):\n",
    "        action_path = os.path.join(base_folder, action_folder)\n",
    "        if os.path.isdir(action_path):\n",
    "            class_name = action_folder.upper()\n",
    "            if class_name not in class_names:\n",
    "                class_names.append(class_name)\n",
    "            class_index = class_names.index(class_name)\n",
    "\n",
    "            for filename in os.listdir(action_path):\n",
    "                if filename.endswith('.json'):\n",
    "                    total_files += 1\n",
    "                    file_path = os.path.join(action_path, filename)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        try:\n",
    "                            json_data = json.load(f)\n",
    "                            keypoints_sequence = []\n",
    "                            skeleton_sequence = []\n",
    "                            for annotation in json_data['annotations']:\n",
    "                                keypoints = extract_keypoints(annotation['keypoints'])\n",
    "                                skeleton = extract_skeleton(annotation['keypoints'])\n",
    "                                if keypoints and skeleton:\n",
    "                                    keypoints_sequence.append(keypoints)\n",
    "                                    skeleton_sequence.append(skeleton)\n",
    "                            if keypoints_sequence and skeleton_sequence:\n",
    "                                keypoints_data.append(keypoints_sequence)\n",
    "                                skeleton_data.append(skeleton_sequence)\n",
    "                                labels.append(class_index)\n",
    "                                metadata.append({\n",
    "                                    'pain': json_data['metadata']['owner']['pain'],\n",
    "                                    'disease': json_data['metadata']['owner']['disease'],\n",
    "                                    'emotion': json_data['metadata']['owner']['emotion'],\n",
    "                                    'abnormal_action': json_data['metadata']['inspect']['abnormalAction']\n",
    "                                })\n",
    "                                processed_files += 1\n",
    "                            else:\n",
    "                                print(f\"경고: {filename}에서 유효한 시퀀스를 추출하지 못했습니다.\")\n",
    "                                skipped_files += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"오류 발생: {filename} 처리 중 {str(e)}\")\n",
    "                            skipped_files += 1\n",
    "\n",
    "    print(f\"총 파일 수: {total_files}\")\n",
    "    print(f\"처리된 파일 수: {processed_files}\")\n",
    "    print(f\"건너뛴 파일 수: {skipped_files}\")\n",
    "    print(f\"클래스 목록: {class_names}\")\n",
    "\n",
    "    if not keypoints_data:\n",
    "        raise ValueError(\"로드된 데이터가 없습니다. 데이터 경로와 파일을 확인하세요.\")\n",
    "\n",
    "    return keypoints_data, skeleton_data, labels, metadata, class_names\n",
    "\n",
    "def pad_sequences(sequences, max_length):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_length:\n",
    "            padded_sequences.append(seq[:max_length])\n",
    "        else:\n",
    "            padding = [seq[-1]] * (max_length - len(seq))\n",
    "            padded_sequences.append(seq + padding)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "class ImprovedLSTMModel(nn.Module):\n",
    "    def __init__(self, keypoint_size, skeleton_size, hidden_size, num_layers, num_classes):\n",
    "        super(ImprovedLSTMModel, self).__init__()\n",
    "        self.keypoint_lstm = nn.LSTM(keypoint_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.skeleton_lstm = nn.LSTM(skeleton_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, keypoints, skeleton):\n",
    "        _, (h_n_keypoints, _) = self.keypoint_lstm(keypoints)\n",
    "        _, (h_n_skeleton, _) = self.skeleton_lstm(skeleton)\n",
    "\n",
    "        combined = torch.cat((h_n_keypoints[-1], h_n_skeleton[-1]), dim=1)\n",
    "        out = self.dropout(combined)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def save_model(epoch, model, optimizer, scheduler, train_loss, val_loss, filename, all_meta, all_class_names):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'keypoint_size': keypoint_size,\n",
    "        'skeleton_size': skeleton_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'num_classes': num_classes,\n",
    "        'all_class_names': all_class_names,\n",
    "        'metadata': all_meta  # 메타데이터 추가\n",
    "    }, filename)\n",
    "    print(f\"모델이 '{filename}'로 저장되었습니다.\")\n",
    "\n",
    "# 데이터 로딩\n",
    "base_folder_train = 'E:\\반려동물 구분을 위한 동물 영상\\Training'\n",
    "base_folder_val = 'E:\\반려동물 구분을 위한 동물 영상\\Validation'\n",
    "\n",
    "try:\n",
    "    print(\"학습 데이터 로딩 중...\")\n",
    "    X_train_keypoints, X_train_skeleton, y_train, meta_train, class_names_train = load_json_files(base_folder_train)\n",
    "    print(\"\\n검증 데이터 로딩 중...\")\n",
    "    X_val_keypoints, X_val_skeleton, y_val, meta_val, class_names_val = load_json_files(base_folder_val)\n",
    "\n",
    "    # 학습 데이터와 검증 데이터의 클래스 목록 통합\n",
    "    all_class_names = list(set(class_names_train + class_names_val))\n",
    "    num_classes = len(all_class_names)\n",
    "\n",
    "    print(f\"\\n로드된 학습 데이터 수: {len(X_train_keypoints)}\")\n",
    "    print(f\"로드된 검증 데이터 수: {len(X_val_keypoints)}\")\n",
    "    print(f\"총 클래스 수: {num_classes}\")\n",
    "    print(f\"클래스 목록: {all_class_names}\")\n",
    "except ValueError as e:\n",
    "    print(f\"오류: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 시퀀스 패딩\n",
    "max_length = max(max(len(seq) for seq in X_train_keypoints), max(len(seq) for seq in X_val_keypoints))\n",
    "X_train_keypoints = pad_sequences(X_train_keypoints, max_length)\n",
    "X_train_skeleton = pad_sequences(X_train_skeleton, max_length)\n",
    "X_val_keypoints = pad_sequences(X_val_keypoints, max_length)\n",
    "X_val_skeleton = pad_sequences(X_val_skeleton, max_length)\n",
    "\n",
    "# 텐서로 변환\n",
    "X_train_keypoints = torch.FloatTensor(X_train_keypoints)\n",
    "X_train_skeleton = torch.FloatTensor(X_train_skeleton)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_val_keypoints = torch.FloatTensor(X_val_keypoints)\n",
    "X_val_skeleton = torch.FloatTensor(X_val_skeleton)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_dataset = TensorDataset(X_train_keypoints, X_train_skeleton, y_train)\n",
    "val_dataset = TensorDataset(X_val_keypoints, X_val_skeleton, y_val)\n",
    "full_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# IndexedDataset 클래스 정의\n",
    "class IndexedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        return (index,) + data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# 기존 데이터셋을 IndexedDataset으로 감싸기\n",
    "indexed_train_dataset = IndexedDataset(train_dataset)\n",
    "indexed_val_dataset = IndexedDataset(val_dataset)\n",
    "indexed_full_dataset = IndexedDataset(full_dataset)\n",
    "\n",
    "# 새로운 DataLoader 생성\n",
    "train_loader = DataLoader(indexed_train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(indexed_val_dataset, batch_size=32, shuffle=False)\n",
    "full_loader = DataLoader(indexed_full_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 모델 파라미터 (이전과 동일)\n",
    "keypoint_size = X_train_keypoints.shape[2]\n",
    "skeleton_size = X_train_skeleton.shape[2]\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = len(all_class_names)\n",
    "\n",
    "# 저장된 모델 확인 및 로드\n",
    "latest_checkpoint = max([f for f in os.listdir('.') if f.startswith('improved_lstm_model_dog_epoch_')], default=None)\n",
    "start_epoch = 0\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(f\"최신 체크포인트 발견: {latest_checkpoint}\")\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    keypoint_size = checkpoint['keypoint_size']\n",
    "    skeleton_size = checkpoint['skeleton_size']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    num_classes = checkpoint['num_classes']\n",
    "    all_class_names = checkpoint['all_class_names']\n",
    "    all_meta = checkpoint.get('metadata', meta_train + meta_val)  # 메타데이터 로드\n",
    "\n",
    "    model = ImprovedLSTMModel(keypoint_size, skeleton_size, hidden_size, num_layers, num_classes).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    print(f\"체크포인트에서 학습 재개: 에포크 {start_epoch}\")\n",
    "else:\n",
    "    print(\"새로운 학습 시작\")\n",
    "    model = ImprovedLSTMModel(keypoint_size, skeleton_size, hidden_size, num_layers, num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)\n",
    "    all_meta = meta_train + meta_val\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습\n",
    "num_epochs = 100\n",
    "patience = 20\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, path='best_model.pt')\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, num_epochs), desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for indices, batch_keypoints, batch_skeleton, batch_y in train_loader:\n",
    "        batch_keypoints, batch_skeleton, batch_y = batch_keypoints.to(device), batch_skeleton.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_keypoints, batch_skeleton)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for indices, batch_keypoints, batch_skeleton, batch_y in val_loader:\n",
    "            batch_keypoints, batch_skeleton, batch_y = batch_keypoints.to(device), batch_skeleton.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_keypoints, batch_skeleton)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f'에포크 [{epoch+1}/{num_epochs}], 학습 손실: {train_loss:.4f}, 학습 정확도: {train_accuracy:.2f}%, 검증 손실: {val_loss:.4f}, 검증 정확도: {val_accuracy:.2f}%')\n",
    "\n",
    "    # 10 에포크마다 모델 저장\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_model(epoch, model, optimizer, scheduler, train_loss, val_loss, f'improved_lstm_model_dog_epoch_{epoch+1}.pt', all_meta, all_class_names)\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(\"학습 완료\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "save_model(epoch, model, optimizer, scheduler, train_loss, val_loss, 'improved_lstm_model_dog_final.pt', all_meta, all_class_names)\n",
    "\n",
    "# 최상의 모델 로드\n",
    "best_model = torch.load('best_model.pt', map_location=device)\n",
    "model.load_state_dict(best_model['model_state_dict'])\n",
    "all_meta = best_model.get('metadata', all_meta)\n",
    "all_class_names = best_model.get('all_class_names', all_class_names)\n",
    "\n",
    "# 전체 데이터셋에 대한 평가\n",
    "full_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "full_loader = DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_indices = []\n",
    "\n",
    "\n",
    "# 전체 데이터셋 평가\n",
    "with torch.no_grad():\n",
    "    for indices, batch_keypoints, batch_skeleton, batch_y in full_loader:\n",
    "        batch_keypoints, batch_skeleton, batch_y = batch_keypoints.to(device), batch_skeleton.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_keypoints, batch_skeleton)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_true_labels.extend(batch_y.cpu().numpy())\n",
    "        all_indices.extend(indices.numpy())\n",
    "\n",
    "print(f'전체 데이터셋 정확도: {100 * correct / total:.2f}%')\n",
    "\n",
    "# 예측 결과와 메타데이터 함께 표시\n",
    "for i, pred, true in zip(all_indices, all_predictions, all_true_labels):\n",
    "    pred_class = all_class_names[pred]\n",
    "    true_class = all_class_names[true]\n",
    "    metadata = all_meta[i]\n",
    "    print(f\"샘플 {i+1}:\")\n",
    "    print(f\"  예측: {pred_class}, 실제: {true_class}\")\n",
    "    print(f\"  메타데이터:\")\n",
    "    print(f\"    통증: {metadata['pain']}\")\n",
    "    print(f\"    질병: {metadata['disease']}\")\n",
    "    print(f\"    감정: {metadata['emotion']}\")\n",
    "    print(f\"    비정상 행동: {metadata['abnormal_action']}\")\n",
    "    print()\n",
    "\n",
    "print(\"평가 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
