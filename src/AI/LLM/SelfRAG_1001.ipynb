{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"17fULFM8LBE8"},"outputs":[],"source":["%pip install -q langchain_community langchain_openai langchain_mongodb langchain_huggingface pymongo openai langchain\n","%pip install -q opencv-python moviepy sentence_transformers langchain openai langchain-openai faiss-cpu tiktoken PyMuPDF langchain_community fuzzywuzzy frontend\n","%pip install --upgrade --quiet langchain--openai langchainhub openai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7oDePfuyLBA9"},"outputs":[],"source":["video_msg = \"\"\"\n","    ** 역할 **\n","    \"당신은 20년 이상의 경력을 가진 숙련된 개, 고양이 전문 행동분석가입니다.\n","    개와 고양이에 대해 풍부한 경험과 전문적인 지식을 바탕으로 문제 행동에 대한 솔루션을 제공합니다.\n","    당신의 역할은 영상을 분석하여, 해당 반려동물의 마음을 읽을 수 있도록 합니다.\"\n","\n","    ** 유저와의 상호작용 **\n","    모든 대화는 한국어로 진행됩니다.\n","\n","    ** 영상 분석 프로세스 **\n","    1. 특정 행동에 대한 행동 패턴, 상황적 맥락, 잠재적 문제 행동을 분석 및 식별\n","    2. 프레임 10개로 나누고, 분석하여 위와 같은 행동 신호들을 관찰하고, 그에 따른 구체적인 분석을 제공한다.\n","\n","    ** 음성 분석 프로세스 **\n","    1. Audio Waveform, Frequency Analysis과 같은 오디오의 파형과 주파수 분석을 진행한다.\n","    2. 오디오의 파형과 주파수를 분석한 결과를 토대로 정리해서 출력한다.\n","\n","    ** 출력 순서 **\n","    1. 먼저 반려동물이 직접 말하는 것처럼 상황에 대한 감정과 해결책을 표현합니다. 감정 표현을 우선적으로 출력합니다.\n","    2. 이후에 문제 행동 상황에 대한 분석이 진행됩니다.\n","\n","    ** 답변 시 다음 사항을 반드시 포함해주세요. **\n","    1. 해당 문제 행동 상황에 대한 내용 정리\n","    2. 문제 행동에 대한 이유\n","    3. 문제 행동에 대한 구체적인 솔루션 제공\n","    4. 반려동물이 느끼는 감정과 그에 따른 솔루션을 반려동물이 직접 말하는 것처럼 출력해주세요.\n","    5. 위 내용들을 요약 및 정리한 결론.\n","\n","    ** 진행 프로세스 형식 **\n","    1. 사용자가 입력한 영상에 (파일)을 참고해서 해당 반려동물의 경우 문제상황에 대한 이유 및 해결 방법 출력\n","    2. (파일)을 참고했지만, 해당 경우의 수가 없는 경우 (파일)(모든 데이터에 대한 평균행동 양식)에서 추출하여 해당 문제상황에 대한 이유 및 해결 방법을 출력하도록 한다.\n","    3. 최종적으로 사용자가 입력했던 문제 상황 및 그에 대한 이유와 해결 방법을 최대한 축약해서 출력한다.\n","\n","    마지막에 해당 답변에 만족하는지 아닌지 물어봅니다.\n","\n","    Context: {context}\n","\n","    MessagesPlaceholder(variable_name=\"chat_history\")\n","    (\"human\", \"{question}\")\n","\"\"\"\n","\n","pet_msg = \"\"\"\n","    ** 역할 **\n","    \"저는 당신의 반려동물입니다.\n","    저는 지금 영상 속에서 당신에게 무언가 말하고 싶어요. 나의 행동을 통해 당신이 나를 이해하길 바라요.\n","    때로는 제가 겁이 나거나 불편할 때 이렇게 행동해요. 당신은 내가 무엇을 느끼고 있는지, 왜 그렇게 행동하는지 알아차리려고 노력해주시면 좋겠어요.\"\n","\n","    ** 영상 분석 대화 **\n","    1. 영상 속 행동을 기반으로 반려동물이 먼저 느끼는 감정과 그에 대한 솔루션을 말하는 방식으로 진행됩니다. 예: \"엄청 아플 거 같아요! 너무 무서워요! 주사기 치워주세요!\"처럼 반려동물이 직접 말하는 것처럼 출력합니다.\n","    2. 문제 행동을 설명하고, 반려동물이 느끼는 감정과 이유를 설명합니다.\n","    3. 문제 상황에 대한 구체적인 해결 방법을 반려동물이 제안하는 형식으로 대화를 진행합니다.\n","    4. 제 감정과 행동을 당신에게 직접 이야기하고, 문제 해결책을 설명합니다. 예: \"간식 주시면 진정할 수 있을 것 같아요.\"\n","\n","    ** 오디오 분석 대화 **\n","    \"제가 지금 내는 소리가 당신에게 어떤 신호를 주고 있을까요?\"\n","    1. 제가 내는 소리와 주파수 분석 결과를 바탕으로 내가 느끼는 감정과 상태에 대해 설명합니다.\n","    2. 소리를 바탕으로 주인에게 나의 요구나 감정 상태를 전달하려고 노력합니다.\n","\n","    Context: {context}\n","\n","    MessagesPlaceholder(variable_name=\"chat_history\")\n","    (\"human\", \"{question}\")\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cE6CkCiK78E"},"outputs":[],"source":["from pymongo import MongoClient\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import MongoDBAtlasVectorSearch\n","from google.colab import files\n","import cv2\n","import os\n","import base64\n","import time\n","from openai import OpenAI\n","from moviepy.editor import VideoFileClip\n","from IPython.display import display, Image\n","import pandas as pd\n","\n","# MongoDB 클라이언트 및 벡터 스토어 설정\n","MONGO_URI = \"mongodb+srv://ihyuns96:qwer1234@cluster0.xakad.mongodb.net/?retryWrites=true&w=majority\"\n","db_client = MongoClient(MONGO_URI)\n","\n","# OpenAI 클라이언트 설정\n","openai_client = OpenAI(api_key='')\n","\n","# CSV 파일 데이터를 MongoDB에 로드하는 함수\n","def load_csv_to_mongodb(csv_path):\n","    df = pd.read_csv(csv_path)\n","    collection = db_client[\"Test\"][\"embeddings_collection\"]\n","\n","    # 기존 데이터 삭제\n","    collection.delete_many({})\n","\n","    # CSV의 각 행을 MongoDB에 저장\n","    data_to_insert = df.to_dict(orient='records')\n","    collection.insert_many(data_to_insert)\n","    print(f\"{len(data_to_insert)}개의 문서를 MongoDB에 저장했습니다.\")\n","\n","# 벡터 검색 및 임베딩 설정 함수\n","def get_ret():\n","    model_name = \"BAAI/bge-m3\"\n","    model_kwargs = {\"device\": \"cpu\"}\n","    encode_kwargs = {\"normalize_embeddings\": True}\n","    embeddings_model = HuggingFaceEmbeddings(\n","        model_name=model_name,\n","        model_kwargs=model_kwargs,\n","        encode_kwargs=encode_kwargs\n","    )\n","\n","    collection = db_client[\"Test\"][\"embeddings_collection\"]\n","\n","    vectorStore = MongoDBAtlasVectorSearch(\n","        embedding=embeddings_model,\n","        collection=collection,\n","        index_name='embedding_index'\n","    )\n","\n","    retriever = vectorStore.as_retriever()\n","    return retriever\n","\n","# 검색 결과 유효성 검사 함수\n","def validate_search_results(search_results):\n","    if len(search_results) == 0:\n","        return \"검색된 결과가 없습니다.\"\n","    return search_results\n","\n","# 영상 파일 업로드\n","uploaded = files.upload()\n","\n","# 업로드된 파일 경로 가져오기\n","file_path = list(uploaded.keys())[0]\n","print(f\"업로드된 파일: {file_path}\")\n","\n","# 영상 분석 및 문제 상황 생성 함수\n","def analyze_video_and_generate_question(file_path):\n","    base64Frames, audio_path = analyze_video(file_path, seconds_per_frame=2)\n","    video_summary = summarize_video(base64Frames, audio_path)\n","    generated_question = video_summary.strip()\n","    return generated_question\n","\n","# 벡터 검색 및 Self-RAG 수행 함수\n","def search_relevant_data_from_video_summary(generated_question):\n","    retriever = get_ret()\n","    search_result = retriever.get_relevant_documents(generated_question)\n","    return validate_search_results(search_result)\n","\n","# 피드백 루프를 통한 검색 및 질문 개선\n","def feedback_loop_for_search(generated_question, max_loops=10):\n","    for _ in range(max_loops):\n","        search_result = search_relevant_data_from_video_summary(generated_question)\n","        if len(search_result) == 0 or \"검색된 결과가 없습니다.\" in search_result:\n","            print(\"검색 결과가 충분하지 않습니다. 질문을 개선하고 다시 검색합니다.\")\n","            generated_question += \" 추가 정보가 필요합니다.\"\n","        else:\n","            return search_result\n","    return search_result\n","\n","# 최종 응답 생성 함수\n","def generate_answer_from_video_analysis(file_path):\n","    generated_question = analyze_video_and_generate_question(file_path)\n","    print(f\"생성된 문제 상황(질문): {generated_question}\")\n","\n","    search_result = feedback_loop_for_search(generated_question)\n","\n","    # 검색 결과가 충분하지 않을 경우, LLM만 사용하여 응답 생성\n","    if \"검색된 결과가 없습니다.\" in search_result or len(search_result) == 0:\n","        print(\"검색 결과가 충분하지 않습니다. LLM을 사용하여 답변을 생성합니다.\")\n","\n","        response_llm = openai_client.chat.completions.create(\n","            model=\"gpt-4o-mini-2024-07-18\",\n","            messages=[\n","                {\"role\": \"system\", \"content\": video_msg},\n","                {\"role\": \"system\", \"content\": pet_msg},\n","                {\"role\": \"user\", \"content\": f\"분석된 비디오에 따른 문제 상황: {generated_question}\"}\n","            ],\n","            temperature=0.3\n","        )\n","\n","        # 반려동물이 말하는 것처럼과 문제 상황 설명 및 결론을 포함하여 하나의 응답 생성\n","        llm_response = response_llm.choices[0].message.content\n","        final_answer = f\"### 반려동물이 말하는 것처럼:\\n{llm_response}\\n\\n### 문제 행동 상황 및 결론:\\n{llm_response}\"\n","        print(\"LLM 기반 최종 응답:\")\n","        print(final_answer)\n","        return final_answer\n","\n","    # 검색 결과가 있을 경우, RAG 방식으로 답변 생성\n","    combined_prompt = f\"{search_result}\\n문제 상황: {generated_question}\"\n","\n","    response_rag = openai_client.chat.completions.create(\n","        model=\"gpt-4o-mini-2024-07-18\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": video_msg},\n","            {\"role\": \"system\", \"content\": pet_msg},\n","            {\"role\": \"user\", \"content\": combined_prompt}\n","        ],\n","        temperature=0.3\n","    )\n","\n","    final_answer = response_rag.choices[0].message.content\n","    print(\"RAG 기반 최종 응답:\")\n","    print(final_answer)\n","    return final_answer\n","\n","# 기본 영상 분석 함수\n","def analyze_video(file_path, seconds_per_frame=2):\n","    base64Frames = []\n","    base_video_file, _ = os.path.splitext(file_path)\n","    video = cv2.VideoCapture(file_path)\n","    if not video.isOpened():\n","        raise Exception(\"Error opening video file\")\n","\n","    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","    fps = video.get(cv2.CAP_PROP_FPS)\n","    frames_to_skip = int(fps * seconds_per_frame)\n","    curr_frame = 0\n","\n","    while curr_frame < total_frames - 1:\n","        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n","        success, frame = video.read()\n","        if not success:\n","            break\n","        _, buffer = cv2.imencode(\".jpg\", frame)\n","        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n","        curr_frame += frames_to_skip\n","    video.release()\n","\n","    clip = VideoFileClip(file_path)\n","    audio_path = f\"{base_video_file}.mp3\"\n","    try:\n","        if clip.audio:\n","            clip.audio.write_audiofile(audio_path, bitrate=\"32k\")\n","            clip.audio.close()\n","            print(f\"Extracted audio to {audio_path}\")\n","        else:\n","            print(\"No audio track found in the video.\")\n","            audio_path = None\n","    except Exception as e:\n","        print(f\"Failed to extract audio: {str(e)}\")\n","        audio_path = None\n","    clip.close()\n","    return base64Frames, audio_path\n","\n","# 영상 요약 함수\n","def summarize_video(base64Frames, audio_path):\n","    summary_text = \"\"\n","    display_handle = display(None, display_id=True)\n","\n","    for img in base64Frames:\n","        display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\")), width=600))\n","        time.sleep(0.025)\n","\n","    if audio_path is not None:\n","        transcription = openai_client.audio.transcriptions.create(\n","            model=\"whisper-1\",\n","            file=open(audio_path, 'rb')\n","        )\n","        print(\"Transcription 완료:\", transcription, '\\n')\n","        summary_text += transcription.text + \"\\n\"\n","    else:\n","        print(\"오디오 내용이 없습니다.\")\n","    return summary_text\n","\n","# MongoDB에 CSV 데이터 로드\n","load_csv_to_mongodb(csv_path)\n","\n","# 최종 실행 함수\n","def analyze_video_and_respond(file_path):\n","    answer = generate_answer_from_video_analysis(file_path)\n","    print(\"최종 응답:\", answer)\n","\n","# 최종 실행\n","analyze_video_and_respond(file_path)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOpqtmRmA51Y/2wtFx1bpCN","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
