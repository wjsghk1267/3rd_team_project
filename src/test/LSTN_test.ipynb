{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (10.2.0)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (3.11.0)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Requirement already satisfied: keras-applications in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications) (1.26.4)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-preprocessing) (1.26.4)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-preprocessing) (1.16.0)\n",
      "Requirement already satisfied: keras-resnet in c:\\programdata\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: keras>=2.2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-resnet) (3.5.0)\n",
      "Requirement already satisfied: absl-py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.2.4->keras-resnet) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.2.4->keras-resnet) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.2.4->keras-resnet) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.2.4->keras-resnet) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.2.4->keras-resnet) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.2.4->keras-resnet) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.2.4->keras-resnet) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.2.4->keras-resnet) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optree->keras>=2.2.4->keras-resnet) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=2.2.4->keras-resnet) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=2.2.4->keras-resnet) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=2.2.4->keras-resnet) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow keras numpy pillow h5py torch opencv-python scikit-learn\n",
    "!pip install -q keras-applications\n",
    "!pip install -q keras-preprocessing\n",
    "!pip install -q keras-resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_3_timestamp_200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_6_timestamp_400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_9_timestamp_600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_18_timestamp_1200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_21_timestamp_1400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_24_timestamp_1600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_27_timestamp_1800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_30_timestamp_2000.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_33_timestamp_2200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_36_timestamp_2400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_39_timestamp_2600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_42_timestamp_2800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_45_timestamp_3000.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_48_timestamp_3200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_51_timestamp_3400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_54_timestamp_3600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_57_timestamp_3800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_60_timestamp_4000.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_63_timestamp_4200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_66_timestamp_4400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_69_timestamp_4600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_72_timestamp_4800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_75_timestamp_5000.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_78_timestamp_5200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_81_timestamp_5400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_84_timestamp_5600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_87_timestamp_5800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_93_timestamp_6200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_96_timestamp_6400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_99_timestamp_6600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_168_timestamp_11200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_171_timestamp_11400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_174_timestamp_11600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_177_timestamp_11800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_180_timestamp_12000.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_183_timestamp_12200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_186_timestamp_12400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_189_timestamp_12600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_192_timestamp_12800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_195_timestamp_13000.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_198_timestamp_13200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_201_timestamp_13400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_204_timestamp_13600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_207_timestamp_13800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_219_timestamp_14600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_222_timestamp_14800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_234_timestamp_15600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_237_timestamp_15800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_240_timestamp_16000.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_243_timestamp_16200.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_246_timestamp_16400.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_249_timestamp_16600.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_252_timestamp_16800.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_255_timestamp_17000.jpg\n",
      "이미지 파일이 없습니다: E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4\\frame_258_timestamp_17200.jpg\n",
      "총 샘플 수: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "테스트 정확도: 1.0\n",
      "\n",
      "메타데이터 예시:\n",
      "{\n",
      "  \"seq\": 156,\n",
      "  \"species\": \"CAT\",\n",
      "  \"action\": \"허리를 아치로 세움\",\n",
      "  \"location\": \"실내\",\n",
      "  \"height\": 1440,\n",
      "  \"width\": 1440,\n",
      "  \"duration\": 17.408,\n",
      "  \"animal\": {\n",
      "    \"breed\": \"코리안 숏헤어\",\n",
      "    \"gender\": \"FEMALE\",\n",
      "    \"age\": 1,\n",
      "    \"neuter\": \"Y\"\n",
      "  },\n",
      "  \"owner\": {\n",
      "    \"pain\": \"N\",\n",
      "    \"disease\": \"N\",\n",
      "    \"emotion\": \"화남/불쾌\",\n",
      "    \"situation\": \"낯선 장소에 있거나 낯선 소리가 날 때\",\n",
      "    \"animalCount\": 1\n",
      "  },\n",
      "  \"inspect\": {\n",
      "    \"action\": \"허리를 아치로 세움\",\n",
      "    \"painDisease\": \"N\",\n",
      "    \"abnormalAction\": \"N\",\n",
      "    \"emotion\": \"화남/불쾌\"\n",
      "  }\n",
      "}\n",
      "\n",
      "행동 클래스:\n",
      "0: 허리를 아치로 세움\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "def load_and_preprocess_data(json_path, image_dir):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metadata = data['metadata']\n",
    "    action = metadata['inspect']['action']\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for annotation in data['annotations']:\n",
    "        frame_number = annotation['frame_number']\n",
    "        timestamp = annotation['timestamp']\n",
    "        image_filename = f\"frame_{frame_number}_timestamp_{timestamp}.jpg\"\n",
    "        image_path = os.path.join(image_dir, image_filename)\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"이미지 파일이 없습니다: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        frame_keypoints = []\n",
    "        for i in range(1, 16):\n",
    "            kp = annotation['keypoints'].get(str(i))\n",
    "            if kp:\n",
    "                frame_keypoints.extend([kp['x'], kp['y']])\n",
    "            else:\n",
    "                frame_keypoints.extend([0, 0])\n",
    "        X.append(frame_keypoints)\n",
    "        y.append(action)\n",
    "    \n",
    "    return np.array(X), y, metadata\n",
    "\n",
    "def prepare_dataset(json_files, image_dirs):\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    for json_file, image_dir in zip(json_files, image_dirs):\n",
    "        X, y, metadata = load_and_preprocess_data(json_file, image_dir)\n",
    "        X_all.extend(X)\n",
    "        y_all.extend(y)\n",
    "        all_metadata.append(metadata)\n",
    "    \n",
    "    X_all = np.array(X_all)\n",
    "    \n",
    "    unique_actions = list(set(y_all))\n",
    "    action_to_index = {action: index for index, action in enumerate(unique_actions)}\n",
    "    y_encoded = [action_to_index[action] for action in y_all]\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    \n",
    "    return X_all, y_categorical, all_metadata, unique_actions\n",
    "\n",
    "# JSON 파일과 이미지 디렉토리 경로 리스트\n",
    "json_files = ['E:/LSTN_test/data/ARCH/20201028_cat-arch-000156.mp4.json']\n",
    "image_dirs = ['E:/LSTN_test/data/img/20201028_cat-arch-000156.mp4']\n",
    "\n",
    "X, y, metadata, action_classes = prepare_dataset(json_files, image_dirs)\n",
    "\n",
    "print(f\"총 샘플 수: {len(X)}\")\n",
    "\n",
    "if len(X) == 0:\n",
    "    print(\"처리할 수 있는 이미지가 없습니다. 프로그램을 종료합니다.\")\n",
    "    exit()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LSTM 모델 구축\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(1, X.shape[1]), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(len(action_classes), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 입력 데이터 reshape\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 모델 평가\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"테스트 정확도: {accuracy}\")\n",
    "\n",
    "# 메타데이터 활용 예시\n",
    "print(\"\\n메타데이터 예시:\")\n",
    "print(json.dumps(metadata[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"\\n행동 클래스:\")\n",
    "for i, action in enumerate(action_classes):\n",
    "    print(f\"{i}: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "테스트 정확도: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "예측 결과:\n",
      "0: 0\n",
      "1: 0\n",
      "2: 0\n",
      "3: 0\n",
      "4: 0\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"테스트 정확도: {accuracy}\")\n",
    "# 모델 예측\n",
    "predictions = model.predict(X_test)\n",
    "predicted_behaviors = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(\"\\n예측 결과:\")\n",
    "for i, behavior in enumerate(predicted_behaviors):\n",
    "    print(f\"{i}: {behavior}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 모델이 'lstm_model.pt' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "def keras_to_pytorch(keras_model):\n",
    "    class PyTorchLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(PyTorchLSTM, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            out, _ = self.lstm(x, (h0, c0))\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "    input_size = keras_model.input_shape[-1]\n",
    "    hidden_size = keras_model.layers[0].units\n",
    "    num_layers = sum(1 for layer in keras_model.layers if isinstance(layer, LSTM))\n",
    "    num_classes = keras_model.output_shape[-1]\n",
    "\n",
    "    pytorch_model = PyTorchLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "    \n",
    "    # Keras 모델의 가중치를 PyTorch 모델로 복사\n",
    "    for i, layer in enumerate(keras_model.layers):\n",
    "        if isinstance(layer, LSTM):\n",
    "            weights = layer.get_weights()\n",
    "            if len(weights) == 3:  # Keras LSTM은 보통 3개의 가중치 배열을 가집니다\n",
    "                # 입력 가중치, 순환 가중치, 편향\n",
    "                input_weights, recurrent_weights, bias = weights\n",
    "                pytorch_model.lstm.weight_ih_l0.data = torch.FloatTensor(input_weights.T)\n",
    "                pytorch_model.lstm.weight_hh_l0.data = torch.FloatTensor(recurrent_weights.T)\n",
    "                # Keras의 편향은 입력 게이트, 망각 게이트, 셀 게이트, 출력 게이트 순서입니다\n",
    "                # PyTorch는 입력 게이트, 망각 게이트, 셀 게이트, 출력 게이트 순서입니다\n",
    "                pytorch_model.lstm.bias_ih_l0.data = torch.FloatTensor(bias[:hidden_size*4])\n",
    "                pytorch_model.lstm.bias_hh_l0.data = torch.FloatTensor(bias[hidden_size*4:])\n",
    "        elif isinstance(layer, Dense):\n",
    "            weights, bias = layer.get_weights()\n",
    "            pytorch_model.fc.weight.data = torch.FloatTensor(weights.T)\n",
    "            pytorch_model.fc.bias.data = torch.FloatTensor(bias)\n",
    "\n",
    "    return pytorch_model\n",
    "\n",
    "# Keras 모델을 PyTorch 모델로 변환\n",
    "pytorch_model = keras_to_pytorch(model)\n",
    "\n",
    "# PyTorch 모델 저장\n",
    "torch.save(pytorch_model.state_dict(), 'lstm_model.pt')\n",
    "\n",
    "print(\"PyTorch 모델이 'lstm_model.pt' 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 상태 키:\n",
      "lstm.weight_ih_l0\n",
      "lstm.weight_hh_l0\n",
      "lstm.bias_ih_l0\n",
      "lstm.bias_hh_l0\n",
      "lstm.weight_ih_l1\n",
      "lstm.weight_hh_l1\n",
      "lstm.bias_ih_l1\n",
      "lstm.bias_hh_l1\n",
      "fc.weight\n",
      "fc.bias\n",
      "\n",
      "각 레이어의 형태:\n",
      "lstm.weight_ih_l0: torch.Size([128, 64])\n",
      "lstm.weight_hh_l0: torch.Size([128, 32])\n",
      "lstm.bias_ih_l0: torch.Size([128])\n",
      "lstm.bias_hh_l0: torch.Size([0])\n",
      "lstm.weight_ih_l1: torch.Size([256, 64])\n",
      "lstm.weight_hh_l1: torch.Size([256, 64])\n",
      "lstm.bias_ih_l1: torch.Size([256])\n",
      "lstm.bias_hh_l1: torch.Size([256])\n",
      "fc.weight: torch.Size([1, 16])\n",
      "fc.bias: torch.Size([1])\n",
      "\n",
      "LSTM 입력 크기: 64\n",
      "LSTM 은닉 상태 크기: 32\n",
      "LSTM 레이어 수: 2\n",
      "출력 크기: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12192\\148980449.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_model = torch.load('lstm_model.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 저장된 모델 로드\n",
    "saved_model = torch.load('lstm_model.pt')\n",
    "\n",
    "# 모델 상태 확인\n",
    "print(\"모델 상태 키:\")\n",
    "for key in saved_model.keys():\n",
    "    print(key)\n",
    "\n",
    "print(\"\\n각 레이어의 형태:\")\n",
    "for key, value in saved_model.items():\n",
    "    print(f\"{key}: {value.shape}\")\n",
    "\n",
    "# LSTM 레이어의 입력 및 은닉 상태 크기 확인\n",
    "if 'lstm.weight_ih_l0' in saved_model:\n",
    "    input_size = saved_model['lstm.weight_ih_l0'].shape[1]\n",
    "    hidden_size = saved_model['lstm.weight_ih_l0'].shape[0] // 4\n",
    "    print(f\"\\nLSTM 입력 크기: {input_size}\")\n",
    "    print(f\"LSTM 은닉 상태 크기: {hidden_size}\")\n",
    "\n",
    "# LSTM 레이어 수 확인\n",
    "num_layers = sum(1 for key in saved_model if key.startswith('lstm.weight_ih_l'))\n",
    "print(f\"LSTM 레이어 수: {num_layers}\")\n",
    "\n",
    "# 출력 레이어(fc) 크기 확인\n",
    "if 'fc.weight' in saved_model:\n",
    "    output_size = saved_model['fc.weight'].shape[0]\n",
    "    print(f\"출력 크기: {output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 이미지: frame_0_timestamp_0.jpg\n",
      "처리된 이미지: frame_12_timestamp_800.jpg\n",
      "처리된 이미지: frame_15_timestamp_1000.jpg\n",
      "처리된 이미지: frame_102_timestamp_6800.jpg\n",
      "처리된 이미지: frame_105_timestamp_7000.jpg\n",
      "처리된 이미지: frame_108_timestamp_7200.jpg\n",
      "처리된 이미지: frame_111_timestamp_7400.jpg\n",
      "처리된 이미지: frame_114_timestamp_7600.jpg\n",
      "처리된 이미지: frame_117_timestamp_7800.jpg\n",
      "처리된 이미지: frame_120_timestamp_8000.jpg\n",
      "처리된 이미지: frame_123_timestamp_8200.jpg\n",
      "처리된 이미지: frame_126_timestamp_8400.jpg\n",
      "처리된 이미지: frame_129_timestamp_8600.jpg\n",
      "처리된 이미지: frame_132_timestamp_8800.jpg\n",
      "처리된 이미지: frame_135_timestamp_9000.jpg\n",
      "처리된 이미지: frame_138_timestamp_9200.jpg\n",
      "처리된 이미지: frame_141_timestamp_9400.jpg\n",
      "처리된 이미지: frame_144_timestamp_9600.jpg\n",
      "처리된 이미지: frame_147_timestamp_9800.jpg\n",
      "처리된 이미지: frame_150_timestamp_10000.jpg\n",
      "처리된 이미지: frame_153_timestamp_10200.jpg\n",
      "처리된 이미지: frame_156_timestamp_10400.jpg\n",
      "처리된 이미지: frame_159_timestamp_10600.jpg\n",
      "처리된 이미지: frame_162_timestamp_10800.jpg\n",
      "처리된 이미지: frame_165_timestamp_11000.jpg\n",
      "동영상이 성공적으로 생성되었습니다: test.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 이미지 파일이 있는 디렉토리 경로\n",
    "이미지_디렉토리 = r'E:\\LSTN_test\\data\\img\\20201028_cat-arch-000156.mp4'\n",
    "\n",
    "# 출력할 동영상 파일 이름\n",
    "출력_비디오 = 'test.mp4'\n",
    "\n",
    "# 프레임 번호를 추출하는 함수\n",
    "def get_frame_number(filename):\n",
    "    match = re.search(r'frame_(\\d+)_', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# 이미지 파일 목록 가져오기 및 정렬\n",
    "이미지_파일들 = [f for f in os.listdir(이미지_디렉토리) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "이미지_파일들.sort(key=get_frame_number)  # 프레임 번호 기준으로 정렬\n",
    "\n",
    "if not 이미지_파일들:\n",
    "    print(\"이미지 파일을 찾을 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 첫 번째 이미지로 비디오 writer 초기화\n",
    "첫_이미지_경로 = os.path.join(이미지_디렉토리, 이미지_파일들[0])\n",
    "첫_이미지 = cv2.imread(첫_이미지_경로)\n",
    "\n",
    "if 첫_이미지 is None:\n",
    "    print(f\"이미지를 열 수 없습니다: {첫_이미지_경로}\")\n",
    "    exit()\n",
    "\n",
    "높이, 너비, _ = 첫_이미지.shape\n",
    "fps = 15  # 초당 프레임 수 설정\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "비디오_writer = cv2.VideoWriter(출력_비디오, fourcc, fps, (너비, 높이))\n",
    "\n",
    "# 각 이미지를 비디오 프레임으로 추가\n",
    "for 이미지_파일 in 이미지_파일들:\n",
    "    이미지_경로 = os.path.join(이미지_디렉토리, 이미지_파일)\n",
    "    프레임 = cv2.imread(이미지_경로)\n",
    "    \n",
    "    if 프레임 is None:\n",
    "        print(f\"이미지를 열 수 없습니다: {이미지_경로}\")\n",
    "        continue\n",
    "    \n",
    "    비디오_writer.write(프레임)\n",
    "\n",
    "# 비디오 writer 해제\n",
    "비디오_writer.release()\n",
    "\n",
    "print(f\"동영상이 성공적으로 생성되었습니다: {출력_비디오}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
